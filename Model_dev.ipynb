{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras import layers, optimizers, losses, activations, models, metrics, initializers, Model, regularizers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_true - y_pred))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(X, y, output_days):\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(layers.LSTM(32, return_sequences=False, activation='relu', recurrent_dropout=0.3, kernel_regularizer=regularizers.l2(1e-4), kernel_initializer='he_normal'))\n",
    "    model.add(layers.Dense(32, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(layers.Dense(output_days, activation='linear', bias_regularizer=regularizers.l2(1e-2)))\n",
    "    model.add(layers.Reshape([output_days, 1]))\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=200, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=30, factor=0.4, min=0.00001)\n",
    "    opt = optimizers.Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=opt, loss=rmse)\n",
    "    model.summary()\n",
    "    \n",
    "    kf = KFold()\n",
    "    loss_histories = []\n",
    "    val_histories = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        history = model.fit(X_train, y_train, shuffle=True,\n",
    "                 validation_data=(X_test, y_test),\n",
    "                 epochs=1000,\n",
    "                 callbacks=[early_stopping, reduce_lr],\n",
    "                 verbose=0)\n",
    "        \n",
    "        loss_histories.append(history.history['loss'])\n",
    "        val_histories.append(history.history['val_loss'])\n",
    "    \n",
    "    return model, [loss_histories, val_histories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attention(X, y, output_days):\n",
    "    \n",
    "    input_layer = layers.Input(shape=(X.shape[1], X.shape[2]))\n",
    "    lstm1 = layers.LSTM(32, activation='relu', return_sequences=True, return_state=True, recurrent_dropout=0.3, kernel_initializer='he_normal')\n",
    "    lstm_output, h_state, c_state = lstm1(input_layer)\n",
    "    attention, att_weights = layers.MultiHeadAttention(num_heads=14, key_dim=32, attention_axes=2) (lstm_output, lstm_output, return_attention_scores=True)\n",
    "    lstm2 = layers.LSTM(32, activation='relu', kernel_initializer='he_normal', recurrent_dropout=0.3, kernel_regularizer=regularizers.l2(1e-6))\n",
    "    lstm2_output = lstm2(attention, initial_state=[h_state, c_state])\n",
    "    dense1 = layers.Dense(32, activation='relu', kernel_initializer='he_normal') (lstm2_output)\n",
    "    dense2 = layers.Dense(7, activation='linear', bias_regularizer=regularizers.l2(1e-3))(dense1)\n",
    "    output_layer = layers.Reshape([output_days, 1])(dense2)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=300, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=30, factor=0.4, min=0.00001)\n",
    "    opt = optimizers.Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=opt, loss=rmse)\n",
    "    model.summary()\n",
    "    \n",
    "    kf = KFold()\n",
    "    loss_histories = []\n",
    "    val_histories = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        history = model.fit(X_train, y_train, shuffle=True,\n",
    "                 validation_data=(X_test, y_test),\n",
    "                 epochs=1000,\n",
    "                 callbacks=[early_stopping, reduce_lr],\n",
    "                 verbose=0)\n",
    "        \n",
    "        loss_histories.append(history.history['loss'])\n",
    "        val_histories.append(history.history['val_loss'])\n",
    "    \n",
    "    return model, [loss_histories, val_histories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C_LSTM(X, y, output_days):\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv1D(32, 3, input_shape=(X.shape[1], X.shape[2]), activation='relu', kernel_regularizer=regularizers.l2(1e-6), padding='same'))\n",
    "    model.add(layers.Conv1D(64, 3, activation='relu', kernel_initializer='he_normal', padding='same'))\n",
    "    model.add(layers.MaxPooling1D(data_format='channels_last'))\n",
    "    model.add(layers.LSTM(32, activation='relu', return_sequences=True, recurrent_dropout=0.3, kernel_initializer='he_normal'))\n",
    "    model.add(layers.LSTM(64, activation='relu', recurrent_dropout=0.3, kernel_initializer='he_normal'))\n",
    "    model.add(layers.Dense(64, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(layers.Dense(output_days, activation='linear', bias_regularizer=regularizers.l2(1e-2)))\n",
    "    model.add(layers.Reshape([output_days, 1]))\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=300, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=30, factor=0.4, min=0.00001)\n",
    "    opt = optimizers.Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=opt, loss=rmse)\n",
    "    model.summary()\n",
    "    \n",
    "    kf = KFold()\n",
    "    loss_histories = []\n",
    "    val_histories = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        history = model.fit(X_train, y_train, shuffle=True,\n",
    "                 validation_data=(X_test, y_test),\n",
    "                 epochs=1000,\n",
    "                 callbacks=[early_stopping, reduce_lr],\n",
    "                 verbose=0)\n",
    "        \n",
    "        loss_histories.append(history.history['loss'])\n",
    "        val_histories.append(history.history['val_loss'])\n",
    "        \n",
    "    return model, [loss_histories, val_histories] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_handling(df, features, required_days, pred_days, test_size=0.1, swap_col= None):\n",
    "    \n",
    "    print(\"##### Reshape and MinMaxScale input #####\")       \n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    tmp = df.copy()\n",
    "    if 'Daily_cases' not in features:\n",
    "        features.append('Daily_cases')\n",
    "        tmp.loc[:, features] = scaler.fit_transform(tmp.loc[:, features])      \n",
    "    else:\n",
    "        tmp.loc[:, features] = scaler.fit_transform(tmp.loc[:, features])\n",
    "    \n",
    "    X = tmp[features]\n",
    "    y = tmp[['Daily_cases']]\n",
    "    if swap_col != None:\n",
    "        X.loc[:, swap_col] = swap_column(X.loc[:, swap_col])\n",
    "        #if \"/\" in swap_col:\n",
    "        #    swap_col = swap_col.replace(\"/\", \"_\")\n",
    "        #X.to_csv('swap_col_{}.csv'.format(swap_col))\n",
    "    X, y = pre_processing(X, y, required_days, pred_days, features)\n",
    "        \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = test_size, shuffle=False, stratify = None)\n",
    "    features.remove('Daily_cases')\n",
    "    \n",
    "    print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(x, y, input_days, output_days, features):\n",
    "    \n",
    "    _output = []\n",
    "    _input = []\n",
    "    \n",
    "    for i in range(len(x)-input_days-output_days+1):\n",
    "        period_end = i + input_days\n",
    "        _input.append(x.iloc[i:period_end, :])\n",
    "        _output.append(y.iloc[period_end:period_end+output_days, :])\n",
    "    _input = np.reshape(_input, (len(x)-input_days-output_days+1, input_days, len(features)))\n",
    "    _output = np.reshape(_output, (len(y)-input_days-output_days+1, output_days, 1))\n",
    "    return _input, _output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_column(df):\n",
    "    index = df.index\n",
    "    if len(df) %2 == 0:\n",
    "        mid = len(df)//2\n",
    "    else:\n",
    "        mid = len(df)//2 + 1\n",
    "    a = df.iloc[:mid].values\n",
    "    b = df.iloc[mid:].values \n",
    "    return pd.DataFrame(np.append(b, a), index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_to_2d(y):\n",
    "    \n",
    "    y_list = y[0]\n",
    "    for i in range(1, len(y)):\n",
    "        tmp = np.reshape(y[i][-1], [1,1])\n",
    "        y_list = np.append(y_list, tmp, axis=0)\n",
    "    return y_list\n",
    "\n",
    "def draw_graph(y_pred_train, y_pred_test, y_true_train, y_true_test, input_days, output_days, country, features, x_train, algo):\n",
    "    \n",
    "    train_start = pd.to_datetime('2020-1-1') + pd.Timedelta(input_days, unit='d')\n",
    "    test_start = pd.to_datetime('2020-1-1') + pd.Timedelta(x_train.shape[0]+input_days, unit='d')\n",
    "    train_rng = pd.date_range(train_start, periods=y_pred_train.shape[0])\n",
    "    test_rng = pd.date_range(test_start, periods=y_pred_test.shape[0])\n",
    "    y_pred_train = y_pred_train.ravel()\n",
    "    y_true_train = y_true_train.ravel()    \n",
    "    y_true_test = y_true_test.ravel()\n",
    "    y_pred_test = y_pred_test.ravel()\n",
    "    \n",
    "    a = pd.DataFrame({'y_train': y_pred_train, 'y_true': y_true_train, 'Date': train_rng})\n",
    "    b = pd.DataFrame({'y_test':y_pred_test, 'y_true': y_true_test, 'Date': test_rng})\n",
    "    \n",
    "    predictions = pd.merge(a, b.iloc[5:, :], how='outer', on=['Date', 'y_true'])\n",
    "    predictions.plot(y=['y_train', 'y_true', 'y_test'], figsize=[20,10], use_index=True)\n",
    "    plt.suptitle(\"Prediction on {}'s daily confirmed cases using {} & {}\".format(country, features, algo), fontsize=20)\n",
    "    plt.savefig('Picture/Prediction_{}_{}_{}.png'.format(algo, country, features))\n",
    "    predictions.to_csv(\"Predictions_{}_{}_{}.csv\".format(algo, country, features))\n",
    "\n",
    "def plot_history(history, country, algo):\n",
    "    \n",
    "    plt.figure(figsize=[20,10])\n",
    "    loss = history[0] \n",
    "    val_loss = history[1]\n",
    "    \n",
    "    time = dt.strftime(dt.now(), \"%Y%m%d %H-%M\")\n",
    "    if not os.path.isdir(os.path.join(os.getcwd(), \"Picture\", time)):\n",
    "        os.mkdir(os.path.join(os.getcwd(), \"Picture\", time))\n",
    "    \n",
    "    for i in range(len(loss)):\n",
    "       \n",
    "        plt.plot(loss[i])\n",
    "        plt.plot(val_loss[i])\n",
    "        plt.title(\"{} Model loss on {}'s data at iteration {}\".format(algo, country, i))\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Test'], loc='upper left')\n",
    "        plt.savefig(\"Picture/{}/Loss_{}_{}_iteration{}.png\".format(time, algo, country, i))\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_development(x_train, x_test, y_train, y_test, f_flag, country, model_path, log_path, required_days, pred_days):\n",
    "\n",
    "    print(\"##### Model Development #####\")\n",
    "    model, history = LSTM_model(x_train, y_train, pred_days)\n",
    "    model2, history2 = Attention(x_train, y_train, pred_days)\n",
    "    model3, history3 = C_LSTM(x_train, y_train, pred_days)\n",
    "    plot_history(history, country, \"LSTM\")\n",
    "    plot_history(history2, country, \"Attention\")\n",
    "    plot_history(history3, country, \"C-LSTM\")\n",
    "    \n",
    "    print(\"##### Results #####\")\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    y_pred_train = compress_to_2d(y_pred_train)\n",
    "    y_true_train = compress_to_2d(y_train)\n",
    "    y_pred_test = model.predict(x_test)\n",
    "    y_pred_test = compress_to_2d(y_pred_test)\n",
    "    y_true_test = compress_to_2d(y_test)\n",
    "    draw_graph(y_pred_train, y_pred_test, y_true_train, y_true_test, required_days, pred_days, country, f_flag, x_train, \"LSTM\")\n",
    "    \n",
    "    \n",
    "    y_pred_train2 = model2.predict(x_train)\n",
    "    y_pred_train2 = compress_to_2d(y_pred_train2)\n",
    "    y_pred_test2 = model2.predict(x_test)\n",
    "    y_pred_test2 = compress_to_2d(y_pred_test2)\n",
    "    draw_graph(y_pred_train2, y_pred_test2, y_true_train, y_true_test, required_days, pred_days, country, f_flag, x_train, \"Attention\")\n",
    "    \n",
    "    y_pred_train3 = model3.predict(x_train)\n",
    "    y_pred_train3 = compress_to_2d(y_pred_train3)\n",
    "    y_pred_test3 = model3.predict(x_test)\n",
    "    y_pred_test3 = compress_to_2d(y_pred_test3)\n",
    "    draw_graph(y_pred_train3, y_pred_test3, y_true_train, y_true_test, required_days, pred_days, country, f_flag, x_train, \"C-LSTM\")\n",
    "    \n",
    "    print(\"##### Logging the metadata #####\")\n",
    "    \n",
    "    mse = tf.keras.losses.MeanSquaredError()\n",
    "    \n",
    "    log_filepath = os.path.join(log_path, \"Models.csv\")\n",
    "    if os.path.isfile(log_filepath):\n",
    "        log = pd.read_csv(log_filepath, index_col=0)\n",
    "    else:\n",
    "        log = pd.DataFrame(columns=[\"Time\", \"Model_path\", \"Algorithm\", \"Loss\", \"Validation Loss\", \"Country\", \"Features\"])\n",
    "    \n",
    "    time = dt.strftime(dt.now(), \"%Y%m%d %H-%M\")\n",
    "    m_path = os.path.join(model_path, time + \"LSTM\")\n",
    "    model.save(m_path)\n",
    "    log = log.append({\"Time\":time, \"Model_path\":m_path, \"Algorithm\":\"LSTM\", \"Loss\": mse(y_true_train, y_pred_train).numpy(), \"Validation Loss\": mse(y_true_test, y_pred_test).numpy(), \"Country\": country, \"Features\": f_flag}, ignore_index=True)\n",
    "    \n",
    "    m_path2 = os.path.join(model_path, time + \"Attention\")\n",
    "    model2.save(m_path2)\n",
    "    log = log.append({\"Time\":time, \"Model_path\":m_path2, \"Algorithm\":\"Attention\", \"Loss\": mse(y_true_train, y_pred_train2).numpy(), \"Validation Loss\": mse(y_true_test, y_pred_test2).numpy(), \"Country\": country, \"Features\": f_flag}, ignore_index=True)\n",
    "    \n",
    "    m_path3 = os.path.join(model_path, time + \"C-LSTM\")\n",
    "    model3.save(m_path3)\n",
    "    log = log.append({\"Time\":time, \"Model_path\":m_path3, \"Algorithm\":\"C-LSTM\", \"Loss\": mse(y_true_train, y_pred_train3).numpy(), \"Validation Loss\": mse(y_true_test, y_pred_test3).numpy(), \"Country\": country, \"Features\": f_flag}, ignore_index=True)\n",
    "    \n",
    "    log.to_csv(log_filepath)\n",
    "    print(\"##### Log completed #####\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(logdir, country, algo='all'):\n",
    "    \n",
    "    log = pd.read_csv(logdir, index_col=0)\n",
    "    if algo=='all':\n",
    "        result = log[(log.Country==country)]\n",
    "    else:\n",
    "        result = log[(log.Country==country) & (log.Algorithm==algo)]\n",
    "    \n",
    "    best_model = result.sort_values('Validation Loss').head(1)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
