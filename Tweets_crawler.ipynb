{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Twitter credentials\n",
    "CONSUMER_KEY = ''\n",
    "CONSUMER_SECRET = ''\n",
    "ACCESS_KEY = ''\n",
    "ACCESS_SECRET = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_accts = {'United States':['WhiteHouse', 'USAGov', 'US_FDA', 'CDCgov'],\n",
    "            'India':['MoHFW_INDIA', 'COVIDNewsByMIB', 'TwitterIndia', 'rashtrapatibhvn', 'MoRD_GOI'],\n",
    "            'Brazil':['govbrazil', 'govbr', 'jairbolsonaro','fabiofaria', 'minsaude'],\n",
    "            'Mexico':['SSalud_mx','GobiernoMX','MexGov_GTA','lopezobrador_','SRE_mx', 'm_ebrard']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Criteria for COVID-19 data\n",
    "general = ['ncov', 'coronavirus', 'covid', 'sars-cov-2', 'New coronavirus', 'unknown pneumonia', 'corona', '2019-nCov', 'COVID-19', 'delta variant',\n",
    "          '#Coronavirus', '#covid19', '#covid', '#corona', '#coronaviras', '#corona-virus', '#covid19-virus', '#sarscov2']\n",
    "key_dict = {'Closure': ['work from home', 'WFH', 'social distance', 'stayhome', 'gatherings restrictions', 'lockdown', 'quarantine', 'reopen',\n",
    "                           '#stayatfomesafe', '#SocialDistancing', '#Quarantine'],\n",
    "                    'Econ':['International support', 'debt relief', 'fiscal measures', 'GDP', 'recession',\n",
    "                           '#stimulus', '#income', '#support', '#Debtrelief', '#Fiscalmeasures', '#export', '#COVID19Economy', '#unemployment'],\n",
    "                    'Vaccination':['vaccine', 'vaccination', 'vaccinating', 'vaccinate', 'immunization', 'covidvaccine', 'covid19vaccine',\n",
    "                                  '#vaccinate', '#GetVaccinated', '#VaccinesWork', '#vaccine'],\n",
    "                    'Health':['testing', 'mask mandate', 'masks', 'mask', 'vaccine development', 'protect elderly',\n",
    "                             '#testing', '#Tracing', '#MaskOn', '#MaskOff', '#PCR', '#PCRTEST', '#MaskMandate']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(CONSUMER_KEY,CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_KEY, ACCESS_SECRET)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_crawler(inputs): \n",
    "    \n",
    "    for country, accts in inputs.items():\n",
    "        print('##### Crawling tweets in ', country, '#####')\n",
    "\n",
    "        for acct in accts:\n",
    "\n",
    "            savingdir = os.path.join(os.getcwd(), 'Data', 'Tweets', country, acct)\n",
    "            if not os.path.isdir(savingdir):\n",
    "                os.mkdir(savingdir)\n",
    "\n",
    "            print('##### Crawling tweets in ', acct, '#####')\n",
    "\n",
    "            db = pd.DataFrame(columns=['username', 'retweets', 'likes', 'text', 'hashtags', 'time'])\n",
    "            list_tweets = []\n",
    "\n",
    "                #tweets = tweepy.Cursor(api.search_full_archive, environment_name='dev', \n",
    "                #                       fromDate='202003010000', toDate='202003302359', query=query, maxResults=num_tweets).items(100)\n",
    "            tweets = api.user_timeline(screen_name = acct, count=200, tweet_mode='extended')\n",
    "            list_tweets.extend(tweets)\n",
    "            #save the id of the oldest tweet less one\n",
    "            oldest = list_tweets[-1].id - 1\n",
    "\n",
    "            #keep grabbing tweets until there are no tweets left to grab\n",
    "            while len(tweets) > 0:\n",
    "                print(f\"getting tweets before {oldest}\")\n",
    "\n",
    "                #all subsiquent requests use the max_id param to prevent duplicates\n",
    "                tweets = api.user_timeline(screen_name = acct, count=200, max_id=oldest, tweet_mode='extended')\n",
    "\n",
    "                #save most recent tweets\n",
    "                list_tweets.extend(tweets)\n",
    "\n",
    "                #update the id of the oldest tweet less one\n",
    "                oldest = oldest = list_tweets[-1].id - 1\n",
    "\n",
    "                print(f\"{len(list_tweets)} tweets downloaded so far\")\n",
    "\n",
    "            for tweet in list_tweets:\n",
    "                time = tweet.created_at\n",
    "                retweets = tweet.retweet_count\n",
    "                likes = tweet.favorite_count\n",
    "                username = tweet.user.screen_name\n",
    "                hashtags = tweet.entities['hashtags']\n",
    "\n",
    "                try:\n",
    "                    text = tweet.retweet_status.full_text\n",
    "                except AttributeError:\n",
    "                    text = tweet.full_text\n",
    "\n",
    "                hashtag = []\n",
    "                for j in range(0, len(hashtags)):\n",
    "                    hashtag.append(hashtags[j]['text'])\n",
    "\n",
    "                record = [username, retweets, likes, text, hashtag, time]\n",
    "                db.loc[len(db)] = record\n",
    "            print(len(db), 'records are returned for', acct)\n",
    "\n",
    "            filename = '{}_tweets.csv'.format(acct)\n",
    "            print('{} written'.format(filename))\n",
    "\n",
    "            db.to_csv(os.path.join(savingdir, filename))\n",
    "        #sleep(900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Crawling tweets in  Mexico #####\n",
      "##### Crawling tweets in  SSalud_mx #####\n",
      "getting tweets before 1436389011556220929\n",
      "400 tweets downloaded so far\n",
      "getting tweets before 1434335477465862144\n",
      "600 tweets downloaded so far\n",
      "getting tweets before 1432146054800097282\n",
      "800 tweets downloaded so far\n",
      "getting tweets before 1429931331522371586\n",
      "1000 tweets downloaded so far\n",
      "getting tweets before 1427644625238609926\n",
      "1200 tweets downloaded so far\n",
      "getting tweets before 1425483420621316109\n",
      "1400 tweets downloaded so far\n",
      "getting tweets before 1423374419762761735\n",
      "1600 tweets downloaded so far\n",
      "getting tweets before 1421266861103230984\n",
      "1800 tweets downloaded so far\n",
      "getting tweets before 1419017037196693507\n",
      "2000 tweets downloaded so far\n",
      "getting tweets before 1417084302240608262\n",
      "2200 tweets downloaded so far\n",
      "getting tweets before 1415030770306007039\n",
      "2400 tweets downloaded so far\n",
      "getting tweets before 1412884380205338628\n",
      "2600 tweets downloaded so far\n",
      "getting tweets before 1411083367580422151\n",
      "2800 tweets downloaded so far\n",
      "getting tweets before 1409323161334726655\n",
      "3000 tweets downloaded so far\n",
      "getting tweets before 1407354589314260993\n",
      "3200 tweets downloaded so far\n",
      "getting tweets before 1405593587098652676\n",
      "3249 tweets downloaded so far\n",
      "getting tweets before 1405196103750979602\n",
      "3249 tweets downloaded so far\n",
      "3249 records are returned for SSalud_mx\n",
      "SSalud_mx_tweets.csv written\n",
      "##### Crawling tweets in  GobiernoMX #####\n",
      "getting tweets before 1432182901555089411\n",
      "400 tweets downloaded so far\n",
      "getting tweets before 1422948504872304639\n",
      "600 tweets downloaded so far\n",
      "getting tweets before 1410623370337652745\n",
      "800 tweets downloaded so far\n",
      "getting tweets before 1401591231453569035\n",
      "1000 tweets downloaded so far\n",
      "getting tweets before 1393302597507813375\n",
      "1200 tweets downloaded so far\n",
      "getting tweets before 1384203404369489926\n",
      "1400 tweets downloaded so far\n",
      "getting tweets before 1372532141838397440\n",
      "1600 tweets downloaded so far\n",
      "getting tweets before 1362852243536564233\n",
      "1800 tweets downloaded so far\n",
      "getting tweets before 1353144835616595967\n",
      "2000 tweets downloaded so far\n",
      "getting tweets before 1342560372147032065\n",
      "2200 tweets downloaded so far\n",
      "getting tweets before 1333935604032299008\n",
      "2400 tweets downloaded so far\n",
      "getting tweets before 1325527080013062144\n",
      "2600 tweets downloaded so far\n",
      "getting tweets before 1318305529958203403\n",
      "2800 tweets downloaded so far\n",
      "getting tweets before 1311272743824887808\n",
      "2999 tweets downloaded so far\n",
      "getting tweets before 1304530421988950018\n",
      "3199 tweets downloaded so far\n",
      "getting tweets before 1298087543007784960\n",
      "3249 tweets downloaded so far\n",
      "getting tweets before 1296507632577110015\n",
      "3249 tweets downloaded so far\n",
      "3249 records are returned for GobiernoMX\n",
      "GobiernoMX_tweets.csv written\n",
      "##### Crawling tweets in  MexGov_GTA #####\n",
      "getting tweets before 808349130330427392\n",
      "13 tweets downloaded so far\n",
      "13 records are returned for MexGov_GTA\n",
      "MexGov_GTA_tweets.csv written\n",
      "##### Crawling tweets in  lopezobrador_ #####\n",
      "getting tweets before 1402692845811781642\n",
      "400 tweets downloaded so far\n",
      "getting tweets before 1359954187816157189\n",
      "600 tweets downloaded so far\n",
      "getting tweets before 1315669910236921855\n",
      "800 tweets downloaded so far\n",
      "getting tweets before 1270324652649267200\n",
      "1000 tweets downloaded so far\n",
      "getting tweets before 1229754394976980992\n",
      "1200 tweets downloaded so far\n",
      "getting tweets before 1188285595686260735\n",
      "1400 tweets downloaded so far\n",
      "getting tweets before 1151934061318983679\n",
      "1600 tweets downloaded so far\n",
      "getting tweets before 1112533283877859332\n",
      "1800 tweets downloaded so far\n",
      "getting tweets before 1074814102667751423\n",
      "2000 tweets downloaded so far\n",
      "getting tweets before 1012079587495481346\n",
      "2200 tweets downloaded so far\n",
      "getting tweets before 953386789624074239\n",
      "2400 tweets downloaded so far\n",
      "getting tweets before 868977671987974143\n",
      "2600 tweets downloaded so far\n",
      "getting tweets before 789161720245723135\n",
      "2800 tweets downloaded so far\n",
      "getting tweets before 685301211814182911\n",
      "3000 tweets downloaded so far\n",
      "getting tweets before 589655261259796479\n",
      "3200 tweets downloaded so far\n",
      "getting tweets before 502632875402215423\n",
      "3250 tweets downloaded so far\n",
      "getting tweets before 484711766115381248\n",
      "3250 tweets downloaded so far\n",
      "3250 records are returned for lopezobrador_\n",
      "lopezobrador__tweets.csv written\n",
      "##### Crawling tweets in  SRE_mx #####\n",
      "getting tweets before 1435724891693764607\n",
      "400 tweets downloaded so far\n",
      "getting tweets before 1430879617263542273\n",
      "600 tweets downloaded so far\n",
      "getting tweets before 1426223715935739911\n",
      "800 tweets downloaded so far\n",
      "getting tweets before 1421473423885209601\n",
      "1000 tweets downloaded so far\n",
      "getting tweets before 1417955924623478785\n",
      "1200 tweets downloaded so far\n",
      "getting tweets before 1413117688835149825\n",
      "1400 tweets downloaded so far\n",
      "getting tweets before 1409574938281975818\n",
      "1600 tweets downloaded so far\n",
      "getting tweets before 1404807941807611914\n",
      "1800 tweets downloaded so far\n",
      "getting tweets before 1398396166170157055\n",
      "2000 tweets downloaded so far\n",
      "getting tweets before 1392828027298516992\n",
      "2200 tweets downloaded so far\n",
      "getting tweets before 1387399187185250307\n",
      "2400 tweets downloaded so far\n",
      "getting tweets before 1379995702965723136\n",
      "2600 tweets downloaded so far\n",
      "getting tweets before 1374096147036315647\n",
      "2800 tweets downloaded so far\n",
      "getting tweets before 1366751185349324801\n",
      "3000 tweets downloaded so far\n",
      "getting tweets before 1360378935100530688\n",
      "3200 tweets downloaded so far\n",
      "getting tweets before 1349511626148331520\n",
      "3250 tweets downloaded so far\n",
      "getting tweets before 1346872271525339135\n",
      "3250 tweets downloaded so far\n",
      "3250 records are returned for SRE_mx\n",
      "SRE_mx_tweets.csv written\n",
      "##### Crawling tweets in  m_ebrard #####\n",
      "getting tweets before 1428886213499527167\n",
      "400 tweets downloaded so far\n",
      "getting tweets before 1419077529240018948\n",
      "600 tweets downloaded so far\n",
      "getting tweets before 1408771754106015745\n",
      "800 tweets downloaded so far\n",
      "getting tweets before 1398356690882514945\n",
      "1000 tweets downloaded so far\n",
      "getting tweets before 1389282722921779201\n",
      "1200 tweets downloaded so far\n",
      "getting tweets before 1377268411407998988\n",
      "1400 tweets downloaded so far\n",
      "getting tweets before 1369750581070430214\n",
      "1600 tweets downloaded so far\n",
      "getting tweets before 1363195492197404675\n",
      "1800 tweets downloaded so far\n",
      "getting tweets before 1352639839758389248\n",
      "2000 tweets downloaded so far\n",
      "getting tweets before 1341162861264035839\n",
      "2200 tweets downloaded so far\n",
      "getting tweets before 1324163032583385092\n",
      "2400 tweets downloaded so far\n",
      "getting tweets before 1309910822517436416\n",
      "2600 tweets downloaded so far\n",
      "getting tweets before 1296845027079946239\n",
      "2800 tweets downloaded so far\n",
      "getting tweets before 1285229822856265730\n",
      "3000 tweets downloaded so far\n",
      "getting tweets before 1273377526740430847\n",
      "3200 tweets downloaded so far\n",
      "getting tweets before 1261376857242398721\n",
      "3250 tweets downloaded so far\n",
      "getting tweets before 1259288326944718849\n",
      "3250 tweets downloaded so far\n",
      "3250 records are returned for m_ebrard\n",
      "m_ebrard_tweets.csv written\n"
     ]
    }
   ],
   "source": [
    "acc_crawler(off_accts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
